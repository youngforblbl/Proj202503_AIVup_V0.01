import json
import requests
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from Chat_api_test import communicate_with_llm  # å¯¼å…¥ LLM äº¤äº’å‡½æ•°
from ChatMEM_api_test import export_and_upload_chat_history  # å¯¼å…¥èŠå¤©å†å²ä¸Šä¼ å‡½æ•°

# è¯¥ç¨‹åºæµ‹è¯•Open-LLM-VTuberçš„åç«¯APIï¼Œä½¿ç”¨openapiã€‚cmdæ‰“å¼€è¯¥ç¨‹åºç«¯å£åç”¨llm_proxy_test.pyä½œä¸ºapiè¾“å…¥ã€‚
# ä½¿ç”¨æ–¹æ³•ï¼ˆcmd)ï¼šuvicorn llm_proxy:app --host 0.0.0.0 --port 8000 --reload

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI()

# é…ç½® API ç›¸å…³å‚æ•°
API_KEY = "[ä½ çš„anythingllm API_key]"  #
WORKSPACE_SLUG = "anythingllm_test"

# âœ… ç»Ÿä¸€ç®¡ç†å…¨å±€å˜é‡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
app.state.chat_count = 0  # è®¡æ•°å™¨ & è®°å½•ä¸Šä¸€è½®çš„ `latest_doc_hash`
app.state.latest_doc_hash = None # ç”¨äºå­˜å‚¨æœ€æ–°çš„æ–‡æ¡£ Hash
TEMP_FILE = "chat_history.json"  # å­˜å‚¨èŠå¤©è®°å½•çš„ JSON æ–‡ä»¶


# OpenAI Chat Completion è¯·æ±‚æ ¼å¼
class ChatRequest(BaseModel):
    model: str
    messages: list  # [{"role": "user", "content": "ä½ çš„é—®é¢˜ï¼Ÿ"}]
    temperature: float = 0.7
    max_tokens: int = 2048
    top_p: float = 1.0
    stream: bool = False


@app.post("/chat")
async def chat_with_llm(request: ChatRequest):
    """
    ç›‘å¬ OpenAI Chat Completion æ ¼å¼çš„ API è¯·æ±‚ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º LLM äº¤äº’ã€‚

    - è§£æç”¨æˆ·è¾“å…¥ï¼ˆå¼¹å¹•å†…å®¹ï¼‰
    - è°ƒç”¨ `communicate_with_llm`
    - ä»¥ OpenAI å…¼å®¹æ ¼å¼è¿”å› LLM å“åº”
    """

    try:
        # è·å–æœ€æ–°çš„ç”¨æˆ·è¾“å…¥
        user_messages = [msg for msg in request.messages if msg["role"] == "user"]
        if not user_messages:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·è¾“å…¥æ¶ˆæ¯ï¼")

        user_input = user_messages[-1]["content"].strip()
        if not user_input:
            raise HTTPException(status_code=400, detail="ç”¨æˆ·è¾“å…¥ä¸ºç©ºï¼")

        # âœ… å¼ºåˆ¶è½¬æ¢å­—ç¬¦ä¸²ç¼–ç 
        user_input = user_input.encode("utf-8").decode("utf-8")

        print(f"ğŸ“¥ æ”¶åˆ°å¼¹å¹•è¾“å…¥: {user_input}")

        # è°ƒç”¨ LLM è¿›è¡Œå¤„ç†
        response = communicate_with_llm(user_input, API_KEY, WORKSPACE_SLUG)
        response = response.encode("utf-8").decode("utf-8")  # âœ… ç¡®ä¿è¿”å›ä¹Ÿæ˜¯ UTF-8

        # è®¡æ•°èŠå¤©æ¬¡æ•°
        app.state.chat_count += 1
        print(app.state.chat_count, app.state.latest_doc_hash)
        # æ¯ 1 æ¬¡èŠå¤©ä¸Šä¼ ä¸€æ¬¡å†å²è®°å½•
        if app.state.chat_count % 1 == 0:
            print("\nğŸ“ 1 æ¬¡å¯¹è¯å·²å®Œæˆï¼Œæ­£åœ¨ä¸Šä¼ èŠå¤©è®°å½•...")
            try:
                upload_status, new_doc_hash = export_and_upload_chat_history(API_KEY, WORKSPACE_SLUG, TEMP_FILE, app.state.latest_doc_hash)
                print(f"âœ…AAA {upload_status}")

                # **æ›´æ–°æœ€æ–°çš„ Hash**
                app.state.latest_doc_hash = new_doc_hash

            except Exception as e:
                print(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")


        print(f"ğŸ“¤ LLM å“åº”: {response}")

        # ç»„è£… OpenAI Chat Completion API å…¼å®¹æ ¼å¼çš„è¿”å›å€¼
        response_payload = {
            "id": "chatcmpl-123",  # å‡è®¾ IDï¼Œå¯æ ¹æ®éœ€æ±‚ä¿®æ”¹
            "object": "chat.completion",
            "created": 1700000000,  # ç”Ÿæˆæ—¶é—´æˆ³
            "model": request.model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": response
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": len(user_input),  # ç²—ç•¥ä¼°ç®—
                "completion_tokens": len(response),  # ç²—ç•¥ä¼°ç®—
                "total_tokens": len(user_input) + len(response)
            }
        }

        return response_payload

    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        raise HTTPException(status_code=500, detail=str(e))
